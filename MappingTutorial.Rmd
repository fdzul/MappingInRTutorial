---
title: "Introduction to Geocoding, Calculating Distances, and Map Making in R"
author: "Sarah Lotspeich, R Ladies Nashville"
date: "September 12, 2017"
output: html_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.align="center", warning=FALSE, message = FALSE)
```
#A few packages for spatial analysis and visualization in R

  1.  [`ggmap`](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf): A collection of functions to visualize spatial data and models
on top of static maps from various online sources (e.g Google Maps and Stamen Maps). It includes tools common to those tasks, including functions for geolocation and routing.
  2.  [`rgdal`](https://cran.r-project.org/web/packages/rgdal/rgdal.pdf): The Geospatial Data Abstraction Library, access to projection/ transformation operations.
  3.  [`GISTools`](https://cran.r-project.org/web/packages/GISTools/GISTools.pdf): Mapping and spatial data manipulation tools -- in particular, drawing choropleth maps. 
  4.  [`ggplot2`](https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf): A system for 'declaratively' creating graphics, based on "The Grammar of Graphics". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.
  5.  [`ggsn`](https://cran.r-project.org/web/packages/ggsn/ggsn.pdf): Adds north symbols (18 options) and scale bars in kilometers to maps in geographic or metric coordinates created with 'ggplot2' or 'ggmap'.
  6.  [`geosphere`](https://cran.r-project.org/web/packages/geosphere/geosphere.pdf): Spherical trigonometry for geographic applications. That is, compute distances and re- lated measures for angular (longitude/latitude) locations.
  7.  [`gmapsdistance`](https://cran.r-project.org/web/packages/gmapsdistance/gmapsdistance.pdf): Get distance and travel time between two points from Google Maps. Four possible modes of transportation (bicycling, walking, driving and public transportation).

```{r, message=FALSE, warning=FALSE}
#install.packages(c("ggmap","rgdal","GISTools","ggplot2","ggsn",geosphere","gmapsdistance"), repos="https://cran.rstudio.com")
library(ggmap)
library(rgdal)
library(GISTools)
library(ggplot2)
library(ggsn)
library(geosphere)
library(gmapsdistance)
```

#Introduction to geocoding

In geospatial analyses, string addresses hold very little meaning just as in our everyday lives the precise latitude and longitude coordinates of our homes are not all that useful. That's where geocoding comes in. The basis of geocoding is the following (respectfully copy-pasted from Google about their [Google Maps Geocoding API](https://developers.google.com/maps/documentation/geocoding/start)): 

"Geocoding is the process of converting addresses (like a street address) into geographic coordinates (like latitude and longitude), which you can use to place markers on a map, or position the map."

Before we begin, here are a few "best practices" when preparing addresses to be geocoded (taken from the [Harvard University Center for Geographic Analysis](http://gis.harvard.edu/services/blog/geocoding-best-practices)): 

  1.  Remove suite/ apartment numbers as they will simply create "ties". 
  2.  Be mindful of special characters like @, #, ?, ;, -
  3.  Buckle down and be prepared to spend a good long while "cleaning" addresses. I spent two days on my pharmacy addresses. I would highly recommend doing it with a quality latte (or perhaps something stronger) in hand.

##Example 1: [City of Austin Public Art Collection](https://data.austintexas.gov/Fun/City-of-Austin-Public-Art-Collection/yqxj-7evp)

*Background:* This file contains the names and address of artworks commissioned through the Austin Art in Public Places Program. Established by the City in 1985, the Art in Public Places (AIPP) program collaborates with local & nationally-known artists to include the history and values of our community into cultural landmarks that have become cornerstones of Austin's identity. The City of Austin was the first municipality in Texas to make a commitment to include works of art in construction projects (taken from [here](http://www.publicartarchive.org/austinaipp)). 

```{r, warning=FALSE}
AustinPublicArt <- read.csv("https://query.data.world/s/93ujuqzxbfjkwnda3y2p8mgv9", header=TRUE, stringsAsFactors=FALSE)
colnames(AustinPublicArt) 
AustinPublicArt$art_full_address <- paste(AustinPublicArt$art_location_street_address,AustinPublicArt$art_location_city,AustinPublicArt$art_location_state,AustinPublicArt$art_location_zip, sep = " ")
unique(AustinPublicArt$art_full_address) #let's see what we're working with here
```

##Keys for cleaning addresses: all hail [`sub()` and `gsub()`](http://rfunction.com/archives/2354)

If you just want to get rid of the first occurrence of something, use: 

  -   Use `sub("What you don't want", "What you do want", "Where you want it")`
  
If you want to get rid of every occurrence of something, use: 

  -   Use `gsub("What you don't want", "What you do want", "Where you want it")`
  
```{r}
AustinPublicArt$art_location_street_address <- gsub(";", "", AustinPublicArt$art_location_street_address) #get rid of special character ";" at the end of each address
AustinPublicArt$art_location_street_address <- sub(".* - ", "", AustinPublicArt$art_location_street_address) #get rid of "Possum Point - " at the beginning
```

Many geocoders exist today (both open-source and proprietary), but the basis is the same. The algorithm begins by taking a complete street address and breaking it down into its component parts. For example, if we wanted to geocode the Vanderbilt Biostatistics Department we would begin with 

2525 West End Avenue, Nashville, TN 37203 $\rightarrow$ 2525 West End Avenue | Nashville | TN | 37203

The geocoder then works its way down the geographic hierarchy that you specify (depending on the scope of your data) to find increasingly more precise coordinates based on matching the address's: 

  1.  State
  2.  City
  3.  Zip code
  4.  Street address

Fortunately, the R community has blessed us with yet another package: `ggmap`! [`ggmap`](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf) contains a quick function called `geocode()` -- tricky to remember, I know -- that will take in a string address and output the latitude and longitude coordinates utilizing the Google Maps API or the Data Science Toolkit online (the choice is yours). 

```{r, message=TRUE}
geocode("2525 West End Avenue Nashville TN 37203", source="dsk") #source from Data Science Toolkit
geocode("2525 West End Avenue Nashville TN 37203", source="google") #source from Google Maps
```

We can see that the geocoded addresses from each source are quite close. I have not (yet) found much information differentiating between the two, so for now I choose to proceed with Google Maps. However, keep in mind that this free service limits you to 2500 queries per day (so make them count!). As I'm writing this tutorial, though, I've realized that the queries run as a R Markdown file knits do not seem to count... 

```{r, cache=TRUE, warning=FALSE, message=FALSE}
AustinLatLong <- geocode(AustinPublicArt$art_full_address, source="google")
```

Now, the unfortunate truth is that we will rarely be able to successfully geocode an entire set of addresses due to either unforgiveable type-os or missing data. In our case, we were unable to precisely pinpoint the location of `r round(mean(is.na(AustinLatLong))*100,1)`$\%$ of the works of art. This calculation comes from:

```{r}
mean(is.na(AustinLatLong))
```

Based on the histograms of the geocoded latitudes and longitudes, there was reason to limit our observations to those with coordinates clustered around the majority. Recall that we are focusing only on the Austin, Texas urban area, so a work of art that is supposedly located more than 1 degree of latitude or longitude away from the median is unrealistic. The rule of thumb is that one degree of latitude is approximately 69 miles, which motivates this exclusion. 

Additionally, since we are seeking only to describe the dispersion of public works of art across Austin, we can focus on the subset that were successfully geocoded (i.e. the latitude and longitude are not NA). 

```{r}
AustinPublicArt$art_location_long <- AustinLatLong[,1]
AustinPublicArt$art_location_lat <- AustinLatLong[,2]
AustinPublicArt <- subset(AustinPublicArt, !is.na(AustinPublicArt$art_location_long))
AustinPublicArt <- subset(AustinPublicArt, AustinPublicArt$art_location_lat < 32)
AustinPublicArt <- subset(AustinPublicArt, AustinPublicArt$art_location_long < -96)
```
```{r, echo=FALSE, fig.align="center", warning=FALSE, message=FALSE}
library(gridExtra)
beforeLong <- ggplot(AustinLatLong, aes(lon)) + geom_histogram(fill="coral1") + ggtitle("Longitude") + xlab("Before Exclusion")
afterLong <- ggplot(AustinPublicArt, aes(art_location_long)) + geom_histogram(fill="coral1") + ggtitle("Longitude") + xlab("After Exclusion")
grid.arrange(beforeLong, afterLong, ncol=2)
beforeLat <- ggplot(AustinLatLong, aes(lat)) + geom_histogram(fill="coral2") + ggtitle("Latitude") + xlab("Before Exclusion")
afterLat <- ggplot(AustinPublicArt, aes(art_location_lat)) + geom_histogram(fill="coral2") + ggtitle("Latitude") + xlab("After Exclusion")
grid.arrange(beforeLat, afterLat, ncol=2)
```

These exclusions leave us with a subset of `r nrow(AustinPublicArt)` works of art, and now that we have geocoded our addresses we are ready to build our first map! 

#It all starts with a shapefile

Now that we have data that we're interested in visualizing on a map, we need a blank canvas. When mapping in R using `GISTools`, this blank canvas is called a ["shapefile."](https://en.wikipedia.org/wiki/Shapefile) This is the same file type used for Geographic Information Systems (GIS) software, so I prefer it because there is an abundance of data available in this format. The first place I look for maps of the United States is the [U.S. Census Bureau.](https://www.census.gov/geo/maps-data/data/tiger-line.html)

Using the [`readOGR()`](https://www.rdocumentation.org/packages/rgdal/versions/1.2-8/topics/readOGR) function in the `rgdal` package, we can read in the shapefile we've downloaded as a usable spatial layer. 

```{r, warning=FALSE, message=FALSE}
UrbanAreasUS <- readOGR(dsn = path.expand("tl_2016_us_uac10/tl_2016_us_uac10.shp"), layer="tl_2016_us_uac10")
class(UrbanAreasUS)
```

What the heck is a `spatial polygons data frame`? 

##Classes of spatial data

  1.  `SpatialPoints`: simply describe locations (with no attributes)
  2.  `SpatialPointsDataFrame`: describes locations + attributes for them
  2.  `SpatialPolygonsDataFrame`: describes locations + shapes + attributes for them
  
We can access the familiar data frame component of the `UrbanAreas_US` spatial polygons data frame as follows: 

```{r}
head(UrbanAreasUS@data) #or alternatively head(data.frame(UrbanAreasUS))
```

By taking advantage of this data frame, we can focus our map on the Austin, TX urban area using our usual data subsetting techniques. Be careful not to select a subset of only the data frame, because if you lose the "spatial polygon" part R will not plot your map properly (i.e. as a map). 

```{r}
UrbanAreasUS@data[which(UrbanAreasUS@data$NAME10 == "Austin, TX"),] #find the unique identifier for Austin, TX
AustinTX <- subset(UrbanAreasUS, UrbanAreasUS@data$NAME10 == "Austin, TX")
```

##Choose your adventure: GISTools or ggplot?

###`GIS Tools`

To plot this shapefile using `GISTools`, our work is basically done. 

```{r, fig.align="center"}
par(mar=c(1,1,1,1))
plot(AustinTX)
```

Now that we have a basic outline of the Austin, TX urban area we can dive into the fun stuff: customization! Feel free to play around with the following aesthetics within your `plot()` function:

  - `col`: select the [fill color](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) for your mapped area
  - `bg`: select the [background color](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) for your map
  - `lty`: choose your [outline style](http://www.statmethods.net/advgraphs/parameters.html)
  - `main`: give your map a main title

In addition, the `map.scale()` and `north.arrow()` functions can add insightful references to a map. The `locator()` function can be helpful in selecting the appropriate x and y to place these ornaments, because it allows you to click the desired area on a plot to find its coordinates.  Additional function inputs are as follows: 

  - `metric`: a boolean, when `metric = TRUE` the output scale will be in miles otherwise the map scale will be in kilometers.
  - `ratio`: a boolean for if you want to display the scale ratio of the map. 
  - `len`: the length (in map units) of the arrow base.

```{r, fig.align="center", warning=FALSE, message=FALSE}
par(mar=c(1,1,1,1))
plot(AustinTX, asp = 1, col="darkorange3", bg="wheat", lty=1, main = "Austin, Texas Urban Area")
# locator()
maps::map.scale(-97.49575, 30.06154, metric=FALSE, ratio = FALSE)
north.arrow(-98.11653, 30.64451, len=1/40, col="white")
```

###`ggplot2`

To map Austin in `ggplot2` instead we need to use the `fortify()` function to convert the spatial polygon data frame to a traditional data frame. For us to be able to subset the exhaustive U.S. Urban Areas to Austin, TX we need to use the region input to keep the `NAME10` unique identifiers.

```{r}
UrbanAreasUS.df <- fortify(UrbanAreasUS, region = "NAME10") #convert spatial polygon data frame --> data frame
AustinTX.df <- subset(UrbanAreasUS.df, UrbanAreasUS.df$id == "Austin, TX")
```

Once the shapefile has been formatted appropriately, we can utilize our knowledge of the "grammar of graphics" to map the urban area for Austin, TX once more. We will need the additional `ggsn` package to add north arrows and map scales for orientation. 

```{r}
ggplot() + geom_polygon(data = AustinTX.df, aes(x = long, y = lat, group=group), fill = "orange") + ggtitle("Ausin, TX Urban Area") + north(AustinTX.df, location = "topleft")  + scalebar(AustinTX.df, location = "bottomright", dist = 10, dd2km = TRUE, model = 'WGS84', st.size=2.5)
```

The map looks a little squished, no? We can fix this by adding `+ coord_equal(ratio=1)` to the end of our graphical equation to keep the plot equally proportioned.

```{r}
ggplot() + geom_polygon(data = AustinTX.df, aes(x = long, y = lat, group=group), fill = "orange") + ggtitle("Ausin, TX Urban Area") + north(AustinTX.df, location = "topleft")  + scalebar(AustinTX.df, location = "bottomright", dist = 10, dd2km = TRUE, model = 'WGS84', st.size=2.5) + coord_equal(ratio=1) #squished no more
```

##Map of the City of Austin's Public Art Collection 
  
First, we need to create a new spatial points data frame so that R knows to acknowledge the geocoded latitude and longitude as coordinates.

```{r}
AustinPublicArt_SP <- SpatialPointsDataFrame(coords = AustinPublicArt[,9:10], data = AustinPublicArt[,1:8])
```

Now we can begin with our base map from above, and overlay it with the locations of works of street art! Before we can overlay our maps, though, we need to make sure that they are projected on to the same coordinate system. The `proj4string()` function will give us some information about the spatial references for each object. Then we will use the [`spTransform()`](https://www.rdocumentation.org/packages/rgdal/versions/1.2-8/topics/spTransform-methods) function to project the spatial polygon data frame we downloaded from Data.World on to the NAD83 coordinate system. 

```{r}
proj4string(AustinTX)
proj4string(AustinPublicArt_SP)
proj4string(AustinPublicArt_SP) <- CRS("+proj=longlat +datum=NAD83") #tell R what the coordinate system should be
AustinPublicArt_SP <- spTransform(AustinPublicArt_SP, CRS(proj4string(AustinTX)))
identical(proj4string(AustinTX), proj4string(AustinPublicArt_SP)) #the shapefile and art data are now on the same coordinate system! 
```

###In `GISTools`

When plotting spatial point data, we have a new selection of aesthetics to play with: 

  - `pch`: [plotting symbol shape](http://www.statmethods.net/advgraphs/parameters.html)
  - `lwd`: line width relative to default

```{r, fig.align="center"}
#recreate our beautiful base map of Austin from above
par(mar=c(1,1,1,1))
plot(AustinTX, asp = 1, col="white", bg="wheat", lty=1, main = "City of Austin Public Art Collection")
maps::map.scale(-97.49575, 30.06154, metric=FALSE, ratio = FALSE)
north.arrow(-98.11653, 30.64451, len=1/40, col="darkorange1")
#add a feature layer with the locations of artworks
plot(AustinPublicArt_SP, add=TRUE, col="darkorange3", lwd=1.5, pch=3)
```

###And in `ggplot2`

We can use the `geom_point()` function with the latitude and longitude coordinates. 

```{r, fig.align="center"}
ggplot() + geom_polygon(data = AustinTX.df, aes(x = long, y = lat, group=group), fill = "white", col="black") + ggtitle("City of Austin Public Art Collection") + north(AustinTX.df, location = "topleft")  + scalebar(AustinTX.df, location = "bottomright", dist = 10, dd2km = TRUE, model = 'WGS84', st.size=2.5) + coord_equal(ratio=1) + theme(legend.position="none") +  geom_point(data = AustinPublicArt,aes(x = art_location_long, y = art_location_lat, color = "orange"))
```

From the map above, we should be able to make some preliminary observations about the spatial distribution of public artwork across the urban area for the city of Austin, Texas. My primary observation was a large cluster of pieces toward the center of the area.

Perhaps policy makers requested this analysis so that they could assess how the City of Austin Public Arts Collection is bringing enrichment to the people of Austin and if the collection is accessible to everyone. How would we answer their question? 

How about estimating the average distance between works of art as a gauge of availability/ accessibility? 

#Calculating distances between places on a map

When we think about distances traveled, how do we measure them? 

  1.  Distance (shortest route possible between two points)
    a.  [Haversine Formula:](http://www.movable-type.co.uk/scripts/latlong.html) calculates the "great-circle" distance between two points (i.e. "as the crow flies"). Performs well even at short distances. 
$$d(i,j) = 2r\text{arcsin}(\text{sin}^2(\frac{\text{lat}_j-\text{lat}_i}{2}) + \text{cos}(\text{lat}_i)\text{cos}(\text{lat}_j)\text{sin}^2(\frac{\text{long}_j - \text{long}_i}{2}))$$
    b.  [Spherical Law of Cosines:](http://www.movable-type.co.uk/scripts/latlong.html) gives reliable estimates down to a few meters. Formula is simpler than Haversine Formula. 
$$d(i,j) = r \cdot \text{arccos}(\text{sin}(\text{lat}_i)\text{sin}(\text{lat}_j) + \text{cos}(\text{lat}_i)\text{cos}(\text{lat}_j)\text{cos}(\text{long}_j-\text{long}_j))$$
    c.  Where $r = 3959$, the radius of the Earth in miles. If we were interested in calculating distances in kilometers or some other unit, we would replace $r$ with the radius of the Earth in the desired output units.   
  2.  Distance (incorporating transportation networks to measure true route between two points)
  3.  Travel time (incorporating transportation networks to measure true route between two points)
  
Which of these measures you choose might depend on the nature or scope of your project, but since our data set is reasonably manageable I thought we could experiment with all three. 

##Using the `geosphere` package for Haversine Formula and Spherical Law of Cosines

The `geosphere` package contains functions `disthaversine()` and `distcosine()` to calculate the distance between either two vectors of latitude/ longitude coordinates or matrices with two columns for latitude/ longitude, using the Haversine Formula and Spherical Law of Cosines, respectively. 

```{r}
#Recall how to access coordinates from a spatialpointsdataframe
head(AustinPublicArt_SP@coords)

#Input: lat/long coordinates of the 1st and 2nd works of art
#Output: distance between 1st and 2nd works of art (in miles)
distHaversine(AustinPublicArt_SP@coords[1,],  AustinPublicArt_SP@coords[2,], r = 3959)

#Input: lat/long coordinates of the 1st vs. all other works of art
#Output: distances between the 1st and all other works of art (in miles)
distHaversine(AustinPublicArt_SP@coords[1,], AustinPublicArt_SP@coords[-1,], r = 3959)
```

We can take the mean of this last command's output to calculate the average distance to other works of art for the first piece in the dataset (our proposed gauge of accessability). To find the average distance from each of the `r nrow(AustinPublicArt_SP@data)` works of art, we can construct a distance matrix using the `distm()` function in `geosphere()` and then take the `colmeans()` of that matrix.

```{r}
DistanceMatrix_Haversine <- distm(data.frame(AustinPublicArt_SP@coords), data.frame(AustinPublicArt_SP@coords), distHaversine)*0.000621371 #convert km --> mi by multipying * 0.000621371
DistanceMatrix_Haversine[1,] #Look at the first row of distance matrix output
diag(DistanceMatrix_Haversine) <- NA #remove 0 values for distance b/t a work of art and itself
AustinPublicArt_SP@data$avg_distance_haversine <- colMeans(DistanceMatrix_Haversine, na.rm=TRUE)
```

Just for fun, let's look at the histogram of average distance from each piece of art to the rest of the collection. 

```{r, echo=FALSE, fig.align="center", message=FALSE}
qplot(AustinPublicArt_SP@data$avg_distance_haversine, xlab="miles", ylab="pieces of art", main="Average Distance between Artwork and the Remainder of Collection", geom="histogram", fill=I("orange"))
```

The inputs for the `distCosine()` function are the same, and the calculations will be similar for most reasonable distances. We could repeat all of the previous steps with the Spherical Law of Cosines simply by substituting `distCosine()` in for `distHaversine()`. 

```{r}
#Input: lat/long coordinates of the 1st and 2nd works of art
#Output: distance between 1st and 2nd works of art (in miles)
distCosine(AustinPublicArt_SP@coords[1,],  AustinPublicArt_SP@coords[2,], r = 3959)
```

##Using the `gmapsdistance` package for travel times and distances based on road networks

Depending on the focus of the spatial analysis, it may be more prudent to incorporate networks of roads and sidewalks to calculate distances or travel times directly rather than "as the crow flies" (the shortest possible distance between two points). With the gmapsdistance package this can be accomplished using the Google Maps API directly in R to utilize Google Maps data to calculate travel distances (output in meters) and times (output in seconds) by a variety of modes (walking, biking, driving, or taking public transportation), at different times of day, and under different traffic conditions ("pessimistic", "optimistic", "best guess"). 

The inputs for the `gmapsdistance()` function are a little different. With this command, the origin and destination can either be named places or pairs of latitude and longitude coordinates of the forms `CITY+STATE` or `LATITUDE+LONGITUDE`, respectively. To simplify the code, I elected to use the `paste()` function to input coordinates in this format without altering the original spatial polygons data frame. 

```{r}
AustinPublicArt_SP@coords[1,] #access one pair of lat/long coordinates from the spatial polygons data frame
paste(AustinPublicArt_SP@coords[1,2],AustinPublicArt_SP@coords[1,1], sep="+") #reformat this pair to be of the form "lat+long"
```

First, you need to quickly get your [unique API key from Google](https://developers.google.com/places/web-service/get-api-key). 

```{r}
set.api.key("MYKEY")
```

Given what we've observed from the preliminary map of the dispersal of public art in Austin, it could be insightful to incorporate sidewalk and road networks to calculate distances for pedestrians rather than cars (as street art could be more easily enjoyed on foot). Let's begin by calculating the walking distance between the first and second works of art in the dataset. 

```{r}
gmapsdistance(origin = paste(AustinPublicArt_SP@coords[1,2],AustinPublicArt_SP@coords[1,1], sep="+"), destination = paste(AustinPublicArt_SP@coords[2,2],AustinPublicArt_SP@coords[2,1], sep="+"), mode = "walking")
```

From this output, we see that there are two accesible objects: the `Time` and `Distance` between the `origin` and `destination`. The following are additional function inputs that could be used to get more precise estimates of these: 
  - `avoid`: when the `mode = "driving"`, we can specify routes avoiding `"tolls"`, `"highways"`, `"ferries"`, and `"indoor"` segments. 
  - `traffic model`: when the `mode = "driving"`, we can specify the anticipated level of traffic using `"optimistic"`, `"pessimistic"`, or `"best_guess"`. 

##Example 2: [Places of Worship](http://opendata.dc.gov/datasets/b134de8f8eaa49499715a38ba97673c8_5)

*Background:* The dataset contains locations and attributes of Places of Worship, created as part of the DC Geographic Information System (DC GIS) for the D.C. Office of the Chief Technology Officer (OCTO) and participating D.C. government agencies. Information provided by various sources identified Places of Worship such as churches and faith based organizations. DC GIS staff geo-processed this data from a variety of sources.

```{r}
PlacesOfWorship <- read.csv("https://query.data.world/s/enqqu329pdhaq78usawjway3a", header=TRUE, stringsAsFactors=FALSE)
colnames(PlacesOfWorship)
PlacesOfWorship_SP <- SpatialPointsDataFrame(coords = PlacesOfWorship[,6:7], data = PlacesOfWorship[,1:5])
head(PlacesOfWorship_SP@data) #view attributes available
```

We begin by repeating the subset process to get our base layer shape for the state of Washington, D.C.. We can find this outline by downloading the [States (and equivalent) shapefile](https://www.census.gov/cgi-bin/geo/shapefiles/index.php) from the United States Census Bureau. Helpful tidbit: the [FIPS code](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code) for the District of Columbia is 11. By using the FIPS code instead of trying to use a string "District of Columbia" we can be careful about capitalizations or abbreviations. 

###In `GISTools`

Using `GISTools`, we can plot this shapefile with the following code: 

```{r, fig.align="center", warning=FALSE, message=FALSE}
States <- readOGR(path.expand("tl_2016_us_state/tl_2016_us_state.shp"), "tl_2016_us_state")
WashingtonDC_shape <- subset(States, States@data$STATEFP == 11)
par(mar=c(0,0,0,0))
plot(WashingtonDC_shape, col="cornflowerblue")
```

###In `ggplot2`

In `ggplot2`, we can alternatively plot it using either of the following commands. The first repeats the steps from the Austin, Texas example to `fortify()` the shapefile and then plot the latitude and longitude coordinates as a polygon using `geom_polygon()`. The second is new! Within `ggplot2`, there is a command called `map_data()` that allows you to directly read in common map shapes. We couldn't use this before because we were focusing on a small area, but now would be a fun time to explore it to plot Washington D.C.! 

```{r, fig.align = "center"}
WashingtonDC_df <- fortify(WashingtonDC_shape, region = "NAME")
DCShapefile <- ggplot() + geom_polygon(data = WashingtonDC_df, aes(x = long, y = lat, group = group), fill = "cornflowerblue") + ggtitle("Washington D.C. (from US Census)") + coord_equal(ratio = 1)
WashingtonDC_gg <- subset(map_data("state"), map_data("state")$region == "district of columbia")
DCMapData <- ggplot() + geom_polygon(data = WashingtonDC_gg, aes(x = long, y = lat, group = region), fill="cornflowerblue") + ggtitle("Washington D.C. (from ggplot)") + coord_equal(ratio = 1)
grid.arrange(DCShapefile, DCMapData, ncol=2)
```

Personally, I prefer the precision of the shapefile to that of the `map_data()` plot. But for maps of larger areas (i.e. all of the states) this is definitely a great option!

##Mapping location and attributes together

To illustrate incorporating attributes with spatial data, we will be creating a map of the places of worship in Washington, D.C. where the religion of each location can be identified from the unique symbol/ color combo.

###In `GISTools` 

To assign our unique symbols to each religion, we are going to add a `Symbol` and `Color` column to the data frame portion of the `PlacesOfWorship_SP` spatial polygon data frame. 

```{r}
# proj4string(WashingtonDC_shape)
# proj4string(PlacesOfWorship_SP)
# proj4string(PlacesOfWorship_SP) <- CRS("+proj=longlat +datum=NAD83") #tell R what the coordinate system should be
# PlacesOfWorship_SP <- spTransform(PlacesOfWorship_SP, CRS(proj4string(WashingtonDC_shape)))
# identical(proj4string(WashingtonDC_shape), proj4string(PlacesOfWorship_SP)) #the shapefile and art data are now on the same coordinate system! 

Religions <- unique(PlacesOfWorship$religion)
Colors <- c("red", "orange", "yellow", "green", "blue")
Symbols <- c(15, 16, 17, 18, 20)

for (i in 1:length(Religions))
{
  PlacesOfWorship_SP@data$Symbol[which(PlacesOfWorship_SP@data$religion == Religions[i])] <- Symbols[i]
  PlacesOfWorship_SP@data$Colors[which(PlacesOfWorship_SP@data$religion == Religions[i])] <- Colors[i]
}
```

```{r, fig.align="center"}
par(mar=c(1,1,1,1))
plot(WashingtonDC_shape, asp=1, col="white", bg="wheat", lty=1, main="Places of Worship by Denomination (Washington, D.C.)")
# maps::map.scale(-76.99345, 38.5940, metric=FALSE, ratio = FALSE)
# north.arrow(-77.76864, 39.348, len=1/40, col="red")
plot(PlacesOfWorship_SP, add=TRUE, col=PlacesOfWorship_SP@data$Color, lwd=1, pch=PlacesOfWorship_SP@data$Symbol)
legend(x = "bottomleft", pch=Symbols, col=Colors, legend=Religions)
```

###In `ggplot2` 

```{r}
ggplot() + geom_polygon(data = WashingtonDC_df, aes(x = long, y = lat, group = group), fill = "white") + ggtitle("Washington D.C. (from US Census)") + coord_equal(ratio = 1) + geom_point(data = PlacesOfWorship, aes(x = latitude, y = longitude, col=factor(religion)))
```

With an abundance of Christian churches, it's difficult to see the distribution of other religions. 

```{r}
ftable(PlacesOfWorship_SP@data$religion)
```

From this quick frequency table, we can see that an overwhelming `r round(ftable(PlacesOfWorship_SP@data$religion)[2]/nrow(PlacesOfWorship_SP@data),2)`$\%$ of the houses of worship in the data set are Christian. Perhaps it would be more insightful to view houses of worship belonging to all other religions on this map individually, and then to look at the spatial distribution of only Christian locations. 

```{r, fig.align="center"}
par(mar=c(1,1,1,1))
plot(WashingtonDC_shape, asp=1, col="white", bg="wheat", lty=1, main="Places of Worship by Denomination (Washington, D.C.)")
# maps::map.scale(-76.99345, 38.5940, metric=FALSE, ratio = FALSE)
# north.arrow(-77.76864, 39.348, len=1/40, col="red")
plot(subset(PlacesOfWorship_SP, PlacesOfWorship_SP@data$religion != "CHRISTIAN"), add=TRUE, col=subset(PlacesOfWorship_SP, PlacesOfWorship_SP@data$religion != "CHRISTIAN")$Color, lwd=3, pch=subset(PlacesOfWorship_SP, PlacesOfWorship_SP@data$religion != "CHRISTIAN")$Symbol, sub = "Excluding Christian")
legend(x = "bottomleft", pch=Symbols[2:5], col=Colors[2:5], legend=Religions[2:5])
```

##Getting colorful with choropleth maps 

A choropleth is a map that shades geographic areas (counties, census blocks, urban areas, etc.) by their relative proportion of a given attribute. Within Washington D.C., we can use a choropleth to examine the relative proportions of Christian houses of worship located in each census block group. To do so, we will be needing the [District of Columbia Block Group shapefile] (https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2016&layergroup=Block+Groups). 

```{r}
CensusBlocks_WashingtonDC <- readOGR(path.expand("tl_2016_11_bg/tl_2016_11_bg.shp"), "tl_2016_11_bg")
```

For R to create the shading for our choropleth map, we need to help it out by tabulating the number of Christian houses of worship in each census block group. We can do this pretty simply by using the poly.count() function, which will count the number of spatial points (from the Christian subset of Places of Worship) that fall inside of each spatial polygon (the shapefile of census block groups).

```{r, warning=FALSE, message=FALSE}
CensusBlocks_WashingtonDC@data$CountChristian <- c(poly.counts(subset(PlacesOfWorship_SP, PlacesOfWorship_SP@data$religion == "CHRISTIAN"), CensusBlocks_WashingtonDC))
```

Now that we have added a "CountChristian" column that tabulates the number of Christian places of worship contained within each census block group we are ready to create our choropleth. 

```{r, fig.align="center"}
par(mar=c(1,1,1,1))
plot(WashingtonDC_shape, asp=1, col="white", bg="wheat", lty=1)
choropleth(CensusBlocks_WashingtonDC, CensusBlocks_WashingtonDC@data$CountChristian, add=TRUE)
title(main="Christian Houses of Worship by Census Block Group")
```

So pretty! But what do the colors mean? Before we add a legend to our beautiful choropleth, we need to talk about shading and color schemes. The choropleth() function is taking the range of counts (from 0 to the most Christian churches observed in a single census block group) and creating five categories. We can create our own custom color scheme using the auto.shading() command with [brewer.pal()](https://cran.r-project.org/web/packages/RColorBrewer/RColorBrewer.pdf) inside of it. brewer.pal() will generate color palettes using the R ColorBrewer package based on a named palette input. Some examples of palettes to chose from include classics like "Reds" (the current default for this choropleth), "Oranges", "Yellows", "Greens", "Blues", "Purples", and "Pinks" as well as  "Accent", "Pastel1", and "Pastel2". 

```{r}
ChristianColors <- auto.shading(CensusBlocks_WashingtonDC@data$CountChristian, cols = brewer.pal(5, "Accent"))
```

Now that we have designated a shading scheme for the choropleth, we can use it to add a legend to our map. This legend gives the colors some meaning in the context of the graphic. 

```{r, fig.align="center"}
# par(mar=c(1,1,1,1))
# plot(WashingtonDC_shape, asp=1, col="white", bg="wheat", lty=1)
# choropleth(CensusBlocks_WashingtonDC, CensusBlocks_WashingtonDC@data$CountChristian, shading = ChristianColors, add=TRUE)
# choro.legend(px = -76.91984, py = 38.845, sh = ChristianColors, cex=0.7)
# title(main="Christian Houses of Worship by Census Block Group")
```
